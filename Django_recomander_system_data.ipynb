{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YassineZITI/Django_DS_Books/blob/master/Django_recomander_system_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Content Based Recommander Sytem That will help to disply similar books to a book.</h3>\n",
        "<p><a href='https://github.com/YassineZITI/IT-or-Not-Books-'>Here</a> i built all external models and data used in this notebook</p>"
      ],
      "metadata": {
        "id": "qoxbJ-NwRMzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md6Zu7m2Nfhw",
        "outputId": "7a6ebdc4-ce63-4e37-b237-ee6f27630054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk --q\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxiQDpbPXPwy",
        "outputId": "aa0d4480-95ff-433b-a4cf-e0a349e2ba99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Create your views here.\n",
        "\n",
        "def preprocess(description):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\tdesc = re.sub('[^A-Za-z]+',' ',description)\n",
        "\tdesc = desc.lower()\n",
        "\t#remove newlines and whitelines\n",
        "\tdesc = re.sub('\\n|\\r',' ',desc)\n",
        "\tdesc = ' '.join([i for i in desc.split(' ') if i not in stop_words])\n",
        "\twith open('/content/drive/My Drive/tokenizer.pkl','rb') as f: \n",
        "\t\ttokenizer = pickle.load(f)\n",
        "\tsequences = tokenizer.texts_to_sequences([desc])\n",
        "\tx = pad_sequences(sequences,maxlen=100,padding='post')\n",
        "\treturn x\n",
        "def EmbeddModel(maxlen,emb_matrix_path):\n",
        "\twith open(emb_matrix_path,'rb') as f:\n",
        "\t\temb_matrix = pickle.load(f)\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(input_dim=emb_matrix.shape[0], output_dim=emb_matrix.shape[1], input_length=maxlen , weights=[emb_matrix]))\n",
        "\t# Average the output of the Embedding layer over the word dimension\n",
        "\tmodel.add(Lambda(lambda x: tf.keras.backend.mean(x, axis=1)))\n",
        "\treturn model\n",
        "\n",
        "def predict(description):\n",
        "\tx = preprocess(description)\n",
        "\tclassifier = load_model('/content/drive/My Drive/classifier.h5') \n",
        "\tembedder = EmbeddModel(100,'/content/drive/My Drive/emb_matrix.pkl')\n",
        "\tembedding = embedder.predict(x)\n",
        "\tprediction = classifier.predict(x)\n",
        "\tit_or_not = ['IT','Not']\n",
        "\tprediction = it_or_not[np.argmax(prediction)]\n",
        "\treturn prediction,embedding\n",
        "\n",
        "def similars(embedding,embeddings_isbn,isbn):\n",
        "\tembed = np.array([i[1] for i in embeddings_isbn])\n",
        "\tindices =  np.argsort(cosine_similarity([embedding], embed))[0][::-1]\n",
        "\tsimilar = ' '.join([i[0] for i in embeddings_isbn[indices[1:9]]])\n",
        "\tembeddings_isbn = np.vstack((embeddings_isbn,np.array((isbn,embedding))))\n",
        "\treturn similar,embeddings_isbn\n"
      ],
      "metadata": {
        "id": "8_XAL8omDoDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_matrix_path = '/content/drive/My Drive/emb_matrix.pkl'"
      ],
      "metadata": {
        "id": "ihPRGNMpD8GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(emb_matrix_path,'rb') as f:\n",
        "\t\temb_matrix = pickle.load(f)"
      ],
      "metadata": {
        "id": "026FMUPfD6u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/app_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "YiC39dsoElNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['Database','TensorFlow','Neural Networks','Hadoop',\n",
        "              'NumPy','Visualization','Machine Learning',\n",
        "              'Analytics','Apache','Big Data','Scala','Microsoft']\n",
        "counts = {c:0 for c in categories}\n"
      ],
      "metadata": {
        "id": "NopGlUtUMERl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = []\n",
        "for book in data:\n",
        "  if counts[book['category']] <=50:\n",
        "    Data.append(book)\n",
        "    counts[book['category']]+=1\n",
        "  "
      ],
      "metadata": {
        "id": "EZBQebxsM34-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_QbEMcNE1PT",
        "outputId": "e0b441f2-ae51-4823-f7ab-5f6d26b348bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "584"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4zUos50WG0Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(Data)):\n",
        "  embedding = predict(Data[i]['description'])[1]\n",
        "  Data[i]['embedding'] = embedding[0]"
      ],
      "metadata": {
        "id": "WTuEWW7d-Gzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.array([(book['isbn13'],book['embedding']) for book in Data])"
      ],
      "metadata": {
        "id": "cW3SCsA5XZfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#let's calculate cosine similarity for sentence 0:\n",
        "cosine_similarity(\n",
        "    [embeddings[0]],\n",
        "    embeddings[1:10]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAJBNTBORchY",
        "outputId": "e9f0337e-ccae-4057-a0af-7254c9da0949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.89961517, 0.8904499 , 0.9000989 , 0.8729072 , 0.85745776,\n",
              "        0.9420755 , 0.76154196, 0.7778647 , 0.9270598 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "book = Data[1]\n",
        "indices =  np.argsort(cosine_similarity([embeddings[1]], embeddings))[0][-9:]\n",
        "\n",
        "indices[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_-X0Rz-Rvxj",
        "outputId": "eb056e68-fc02-431b-aa11-bec8594f79ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 39, 256, 479,  11, 422,   6, 378,   3])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Data)) :\n",
        "  book = Data[i]\n",
        "  indices =  np.argsort(cosine_similarity([embeddings[i]], embeddings))[0][::-1]\n",
        "  \n",
        "  similar = ''\n",
        "\n",
        "  for ind in indices[1:]:\n",
        "    if Data[ind]['category'] == book['category']:\n",
        "      similar += Data[ind]['isbn13'] + ' '\n",
        "      if len(similar) >= 112:\n",
        "        break\n",
        "  book['similar'] = similar\n",
        "  Data[i] = book"
      ],
      "metadata": {
        "id": "irZ4qs0gRzRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data[300]['similar']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EsLtaytsufYp",
        "outputId": "a3eb5aee-9fc3-4ae2-ccc8-e7cd21249085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9781098115784 9781788390040 9781785882951 9781788474399 9781484234310 9781788996402 9781484280164 9781784393908 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/embed.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings,f)"
      ],
      "metadata": {
        "id": "qUMhKxsVMYq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Django-recomander-system-data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6osVIlhkRybjVSILcLh0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}